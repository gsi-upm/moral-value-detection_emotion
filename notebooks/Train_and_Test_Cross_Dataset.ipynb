{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d9d5d-3a28-4ae2-b8a8-3b7198920f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer,AutoModelForSequenceClassification\n",
    "\n",
    "%run Balance_Data.ipynb\n",
    "%run Preprocessing_Functions.ipynb\n",
    "%run TRAIN_AND_TEST.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8e615-497d-4fe2-9c15-6d865ffae9c0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f110fc8-e845-4160-b33f-91d34eff4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MPress task Train and Test Function Cross-Dataset\n",
    "\n",
    "def train_test_binary_crossdataset(df_train, df_tests, model_name, directory, train_name,test_names,task, prompt,lexicon,undersampling=False):\n",
    "    #-----info-----\n",
    "    id2label = {0: \"NO-MORAL\", 1: \"MORAL\"}\n",
    "    label2id = {\"NO-MORAL\": 0, \"MORAL\": 1}\n",
    "    print('START TRAINING:\\n')\n",
    "    print('INFO:')\n",
    "    print(f'Train dataset  {train_name}\\n')\n",
    "    print(f'Lexicon: {lexicon}\\n')\n",
    "    print(f'Prompt: {prompt}\\n')\n",
    "    print(f'Moral: {task}\\n')\n",
    "    print(f'Directory: {directory}\\n')\n",
    "    print(f'Undersampling: {undersampling}\\n')\n",
    "\n",
    "\n",
    "    #-----data preparation-----\n",
    "    df_train = preprocess_data_binary(df_train, task)\n",
    "    logging.info(f'Train examples of {train_name}: {df_train.label.value_counts()}')\n",
    "    for _ in test_names:\n",
    "        df_tests[_]= preprocess_data_binary(df_tests[_], task)\n",
    "\n",
    "    if undersampling:\n",
    "        df_train=undersampling_data(df_train)\n",
    "        for _ in test_names:\n",
    "            df_tests[_]= undersampling_data(df_tests[_])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    #-----split data-----\n",
    "    train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "    print(f'{train_df.shape},{train_name}')\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df)}\n",
    "    for _ in test_names: \n",
    "        datasets[_] = Dataset.from_pandas(df_tests[_])\n",
    "    datasets = DatasetDict(datasets)\n",
    "\n",
    "\n",
    "    \n",
    "    #----model-----\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    \n",
    " \n",
    "  \n",
    "    #-----train----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    \n",
    "    print('Training...')\n",
    "    #start_time_all_data = time.time()\n",
    "    trainer.train()\n",
    "    #end_time_all_data = time.time()\n",
    "    #end_time_all_data = (end_time_all_data - start_time_all_data) \n",
    "    #print(f'Finished training for {train_name}. Time: {end_time_all_data} seg.\\n')\n",
    "    \n",
    "    #-----inference-----\n",
    "    print('Testing...')\n",
    "    print(f'Datasets for test {test_names}')\n",
    "    for _ in test_names:\n",
    "        print(f'Test dataset: {_}')\n",
    "        predictions = trainer.predict(tokenized_datasets[_])\n",
    "        predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "        actual_labels = tokenized_datasets[_][\"label\"]\n",
    "        results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "        file_name = f\"result_{train_name}_{_}_{lexicon}_moral{task}_{prompt}\"\n",
    "        \n",
    "         #-----save metrics-----   \n",
    "        results_directory = os.path.join(directory, file_name)\n",
    "        os.makedirs(results_directory, exist_ok=True)\n",
    "        print(f'File name: {file_name}')\n",
    "\n",
    "        \n",
    "        with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "            json.dump(results, f)\n",
    "            print(f'Finished test for {_}.\\n')\n",
    "    print(f'FINISH! TRAIN DATASET : {train_name}\\n')\n",
    "    print('-----------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d4d76-78a2-4f1e-88b1-76fc418228ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiPress task Train and Test Function Cross-Dataset\n",
    "def train_test_multiclass_crossdataset(df_train, df_tests, model_name, directory, train_name,test_names, prompt,lexicon,undersampling=False):\n",
    "    start_time = time.time()\n",
    "    #-----prepare data-----\n",
    "    print('START TRAINING:\\n')\n",
    "    print('INFO:')\n",
    "    print(f'Train dataset  {train_name}\\n')\n",
    "    print(f'Lexicon: {lexicon}\\n')\n",
    "    print(f'Prompt: {prompt}\\n')\n",
    "    print(f'Directory: {directory}\\n')\n",
    "    print(f'Undersampling: {undersampling}\\n')\n",
    "\n",
    "    id2label,label2id = multiclass_task_5()\n",
    "    print('labels :' )\n",
    "    print(label2id)\n",
    "\n",
    "    #pre-procesar datasets de train y test - según la moral\n",
    "    df_train = label_multiclass5(df_train)\n",
    "    for _ in test_names:\n",
    "        df_tests[_]= label_multiclass5(df_tests[_])\n",
    "\n",
    "\n",
    "    #balancear datos si undersampling=True (varía según el rasgo moral)\n",
    "    if undersampling:\n",
    "        df_train=undersampling_data(df_train)\n",
    "        logging.info(f'Train examples: {df_train.label.value_counts()}')\n",
    "        for _ in test_names:\n",
    "            df_tests[_]= undersampling_data(df_tests[_])\n",
    "            logging.info(f'Test examples of {_} {df_tests[_].label.value_counts()}')\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    logging.info('')\n",
    "\n",
    "    \n",
    "        #guardar los datos en objeto Dataset \n",
    "    train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "    print(f'{train_df.shape},{train_name}')\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df)}\n",
    "    for _ in test_names: \n",
    "        datasets[_] = Dataset.from_pandas(df_tests[_])\n",
    "\n",
    "    #tokenizar\n",
    "    datasets = DatasetDict(datasets)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "    #-----train-----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "        )\n",
    "    logging.info('Training...')\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    print('Testing...')\n",
    "    #-----test-----\n",
    "    print(test_names)\n",
    "    for _ in test_names:\n",
    "        print(f'Test dataset: {_}')\n",
    "        logging.info(f'Test dataset: {_}')\n",
    "        predictions = trainer.predict(tokenized_datasets[_])\n",
    "        predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "        actual_labels = tokenized_datasets[_][\"label\"]\n",
    "        results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "        file_name = f\"result_multiclass_{train_name}_{_}_{lexicon}_{prompt}\"\n",
    "        results_directory = os.path.join(directory, file_name)\n",
    "        os.makedirs(results_directory, exist_ok=True)\n",
    "        print(f'file: {file_name}')\n",
    "        logging.info('')\n",
    "        #-----guardar resultados-----\n",
    "        \n",
    "        with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "            json.dump(results, f)\n",
    "            print(f'Finished test for {_}.\\n')\n",
    "            logging.info(f'Finished test for {_}')\n",
    "            logging.info('')\n",
    "        \n",
    "    print(f'FINISH! TRAIN DATASET : {train_name}\\n')\n",
    "    print('-----------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fec4e-566d-445c-8e61-1487b9be81b5",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db7d06-9034-4a29-b144-5e808c9c9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='Results_Cross_Dataset'\n",
    "undersampling=True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = directory,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='no')\n",
    "\n",
    "\n",
    "#Select model\n",
    "models=['bert-base-uncased','roberta-base']\n",
    "model_name=models[1]\n",
    "\n",
    "#select prompt\n",
    "prompts=['noprompt',1,2,3,4]\n",
    "prompt_number=prompts[2]\n",
    "\n",
    "#select lexicon\n",
    "lexicons=['baseline','moralstrength','depechemood','moralstrength+depechemood']\n",
    "lexicon=lexicons[3]\n",
    "\n",
    "\n",
    "#model tokenizer\n",
    "if model_name=='bert-base-uncased':\n",
    "    folder='Bert'\n",
    "elif model_name=='roberta-base':\n",
    "    folder='Roberta' \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3f7dc-2c76-4cc5-b0b3-0ada2661f371",
   "metadata": {},
   "source": [
    "# MPress Cross-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80743629-432c-4811-9d42-dc1342311f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "folder_path='DATASETS'\n",
    "if model_name=='roberta-base':\n",
    "    if undersampling==True:\n",
    "        directory=('Results_Cross_Dataset/Balanced/Roberta_Moralstrength')\n",
    "    elif undersampling==False:\n",
    "        directory=('Results_Cross_Dataset/Original/Roberta_Moralstrength')\n",
    "\n",
    "\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('_dataset.csv')]\n",
    "test_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352c6b2-d0c4-41d4-9c64-d5e98c0156f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test\n",
    "for f in files:\n",
    "    df_train = pd.read_csv(os.path.join(folder_path, f))\n",
    "    train_name=f.replace('_dataset.csv', '')\n",
    "    print(f'TRAIN {train_name}\\n')\n",
    "    print(f'Data example of the original dataset: {df_train.text.loc[0]}')\n",
    "    # Dictionary of test dataframes\n",
    "    df_tests = {}\n",
    "    files2=[]\n",
    "    \n",
    "    for i,elemento in enumerate(files):\n",
    "        if elemento != f: \n",
    "            files2.append(elemento)\n",
    "            \n",
    "    print(f'Test Data: {files2}\\n')\n",
    "    test_names=[]\n",
    "    for f2 in files2:  \n",
    "        # Load only test dataframes from remaining CSV files\n",
    "        df_test = pd.read_csv(os.path.join(folder_path, f2))\n",
    "        df_tests[f2.replace('_dataset.csv', '')] = df_test\n",
    "        test_names.append(f2.replace('_dataset.csv', ''))\n",
    "\n",
    "    print(f'KEYS {df_tests.keys()}\\n')\n",
    "    print('-------------------------------')\n",
    "            \n",
    "    #Adapt lexicon and prompt\n",
    "    df_train=  prompt_template(df_train, prompt_number,lexicon)\n",
    "    print(f'Data example of the dataset: {df_train.text.loc[0]}')\n",
    "    print(test_names)\n",
    "    for _ in test_names:\n",
    "        df_tests[_]= prompt_template(df_tests[_], prompt_number,lexicon)\n",
    "\n",
    "    #Apply function\n",
    "    for task in range(1, 6):\n",
    "        train_test_binary_crossdataset(df_train.copy(), df_tests.copy(), model_name, directory, train_name,test_names,task,prompt_number,lexicon,undersampling=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165f4ba-785c-49d3-9bf7-e5c610497bb4",
   "metadata": {},
   "source": [
    "# MultiPres Cross Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd4f20-50a6-475d-8d56-80e7fc9ab282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "folder_path='DATASETS'\n",
    "if model_name=='roberta-base':\n",
    "    if undersampling==True:\n",
    "        directory=('Results_Cross_Dataset/Balanced/Roberta_Moralstrength')\n",
    "    elif undersampling==False:\n",
    "        directory=('Results_Cross_Dataset/Original/Roberta_Moralstrength')\n",
    "\n",
    "\n",
    "files = [file for file in os.listdir(folder_path) if file.endswith('_dataset.csv')]\n",
    "test_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3ba61-1328-4ab2-bf13-0cd599412a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test\n",
    "for f in files:\n",
    "    df_train = pd.read_csv(os.path.join(folder_path, f))\n",
    "    train_name=f.replace('_dataset.csv', '')\n",
    "    print(f'TRAIN {train_name}\\n')\n",
    "    print(f'Data example of the original dataset: {df_train.text.loc[0]}')\n",
    "    # Dictionary of test dataframes\n",
    "    df_tests = {}\n",
    "    files2=[]\n",
    "    \n",
    "    for i,elemento in enumerate(files):\n",
    "        if elemento != f: \n",
    "            files2.append(elemento)\n",
    "\n",
    "    print(f'Datos de test: {files2}\\n')\n",
    "    test_names=[]\n",
    "    for f2 in files2:  \n",
    "        # Load only test dataframes from remaining CSV files\n",
    "        df_test = pd.read_csv(os.path.join(folder_path, f2))\n",
    "        df_tests[f2.replace('_dataset.csv', '')] = df_test\n",
    "        test_names.append(f2.replace('_dataset.csv', ''))\n",
    "\n",
    "    print(f'KEYS {df_tests.keys()}\\n')\n",
    "    print('-------------------------------')\n",
    "            \n",
    "    df_train=  prompt_template(df_train, prompt_number,lexicon)\n",
    "    print(f'Data example of the dataset: {df_train.text.loc[0]}')\n",
    "    print(test_names)\n",
    "    for _ in test_names:\n",
    "        df_tests[_]= prompt_template(df_tests[_], prompt_number,lexicon)\n",
    "\n",
    "    #entrenar para cada moral \n",
    "    train_test_multiclass_crossdataset(df_train.copy(), df_tests.copy(), model_name, directory, train_name,test_names,prompt_number,lexicon,undersampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
