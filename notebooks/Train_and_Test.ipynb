{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340d9d5d-3a28-4ae2-b8a8-3b7198920f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer,AutoModelForSequenceClassification\n",
    "\n",
    "%run Balance_Data.ipynb\n",
    "%run Preprocessing_Functions.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38773a1-a914-4dc9-a18f-26ee9b012c93",
   "metadata": {},
   "source": [
    "# Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52a870-cd2b-4040-97ae-8dcde3bdf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPres functions\n",
    "def preprocess_data_binary(df, m):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataframe based on the binary task (MPres).\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing the data to preprocess.\n",
    "    m (int): Moral identifier (1...5)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    if m == 1:\n",
    "        df = labels_m1(df)\n",
    "    elif m == 2:\n",
    "        df = labels_m2(df)\n",
    "    elif m == 3:\n",
    "        df = labels_m3(df)\n",
    "    elif m == 4:\n",
    "        df = labels_m4(df)\n",
    "    elif m == 5:\n",
    "        df = labels_m5(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# MultiPres functions\n",
    "def preprocess_data_polarity(df, mp):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataframe based on the polarity task (MultiPres).\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing the data to preprocess.\n",
    "    mp (int): Moral trait identifier (1...5)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    if mp == 1:\n",
    "        df=label_mp1(df)\n",
    "        id2label, label2id = mp_label1()\n",
    "    elif mp == 2:\n",
    "        df=label_mp2(df)\n",
    "        id2label, label2id = mp_label2()\n",
    "    elif mp == 3:\n",
    "        df=label_mp3(df)\n",
    "        id2label, label2id = mp_label3()\n",
    "    elif mp == 4:\n",
    "        df=label_mp4(df)\n",
    "        id2label, label2id = mp_label4()\n",
    "    elif mp == 5:\n",
    "        df=label_mp5(df)\n",
    "        id2label, label2id = mp_label5()\n",
    "    return df,id2label,label2id\n",
    "\n",
    "\n",
    "# METRICS\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall}\n",
    "\n",
    "# TOKENIZER\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs=tokenizer(examples[\"text\"], truncation=True)\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db15b4dd-863f-4b28-9301-743033c1e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAMES\n",
    "def annotator_dataframes(folder_path, ending):\n",
    "    \"\"\"\n",
    "    Reads CSV files from a specified folder and creates pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing CSV files.\n",
    "    ending (str): File extension or ending of the CSV files to be read (e.g., '.csv').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are filenames (without `ending`) and values are corresponding pandas DataFrames.\n",
    "    list: List of all filenames that matched the specified `ending`.\n",
    "    list: List of filenames without the specified `ending`.\n",
    "    \"\"\"\n",
    "    files = os.listdir(folder_path)\n",
    "    dataframes = {}\n",
    "    final_filenames = []\n",
    "    names = []  \n",
    "    for filename in files:\n",
    "        if filename.endswith(ending):\n",
    "            df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            dataframes[filename.replace(ending, '')] = df\n",
    "            final_filenames.append(filename)\n",
    "            names.append(filename.replace(ending, ''))\n",
    "    return dataframes, final_filenames, names\n",
    "\n",
    "\n",
    "\n",
    "#LOAD DATA\n",
    "def load_data(prompt_number,folder_path,lexicon,ending):\n",
    "    \"\"\"\n",
    "    Loads datasets from CSV files in a specified folder, applies a prompt template based on `prompt_number` and `lexicon`,\n",
    "    and returns modified datasets.\n",
    "\n",
    "    Args:\n",
    "    prompt_number (int): Number indicating the type of prompt template to apply (1, 2, 3, or 4).\n",
    "    folder_path (str): Path to the folder containing CSV files.\n",
    "    lexicon (str): Type of lexicon to use ('moralstrength', 'depechemood', or 'moralstrength+depechemood').\n",
    "    ending (str): File extension or ending of the CSV files to be read (e.g., '.csv').\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are dataset names and values are corresponding pandas DataFrames modified by `prompt_template`.\n",
    "    list: List of all filenames that matched the specified `ending`.\n",
    "    list: List of filenames without the specified `ending`.\n",
    "    \"\"\"\n",
    "    #no prompt\n",
    "    datasets,filenames,names = annotator_dataframes(folder_path,ending)\n",
    "    for dataset in names:\n",
    "        datasets[dataset]=  prompt_template(datasets[dataset], prompt_number,lexicon)\n",
    "  \n",
    "    return datasets,filenames,names\n",
    "\n",
    "\n",
    "#PROMPT AND DATA FUNCTIONS\n",
    "def prompt_template(df, prompt_number,lexicon):\n",
    "    \"\"\"\n",
    "    Modifies the 'text' column of the dataframe based on the specified prompt number and lexicon.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing the data to be processed.\n",
    "    prompt_number (int): Number indicating the prompt template to apply (1 to 4).\n",
    "    lexicon (str): Lexicon type ('moralstrength', 'depechemood', 'moralstrength+depechemood').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with the 'text' column modified according to the specified prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    if lexicon=='moralstrength':\n",
    "        if prompt_number == 1:\n",
    "            df[\"text\"] = df[\"moralstrength\"]  + \" : \" + df[\"text\"]                       # PROMPT 1   \n",
    "        elif prompt_number == 2:\n",
    "            df[\"text\"] = \"the text '\" + df[\"text\"] + \"' reflects the moral value \"+ df[\"moralstrength\"] # PROMPT 2\n",
    "        elif prompt_number == 3:\n",
    "            df[\"text\"] = \"the moral value \" + df[\"moralstrength\"]+ \" is reflected in the text '\" + df[\"text\"] + \" '\"   # PROMPT 3\n",
    "        elif prompt_number == 4:\n",
    "            df[\"text\"] = \"The text '\" + df[\"text\"]+ \"reflects varying intensities of morality such as: \" +df['moralstrength_i']         # PROMPT 4\n",
    "\n",
    "    elif lexicon=='depechemood':\n",
    "        if prompt_number == 1:\n",
    "            df[\"text\"] = df[\"emotion_word\"]  + \" : \" + df[\"text\"]                       # PROMPT 1   \n",
    "        elif prompt_number == 2:\n",
    "            df[\"text\"] = \"the text '\" + df[\"text\"] + \"' reflects the emotion \"+ df[\"emotion_word\"] # PROMPT 2\n",
    "        elif prompt_number == 3:\n",
    "            df[\"text\"] = \"the emotion \" + df[\"emotion_word\"]+ \" is reflected in the text '\" + df[\"text\"] + \" '\"   # PROMPT 3\n",
    "        elif prompt_number == 4:\n",
    "            df[\"text\"] = \"The text '\" + df[\"text\"]+ \"' reflects different emotions such as: \" +df['depechemood']         # PROMPT 4\n",
    "    \n",
    "    elif lexicon=='moralstrength+depechemood':\n",
    "        if prompt_number == 1:\n",
    "            df[\"text\"] = df[\"moralstrength\"] +\" and \"+ df[\"emotion_word\"]  + \" : \" + df[\"text\"]                       # PROMPT 1   \n",
    "        elif prompt_number == 2:\n",
    "            df[\"text\"] = \"the text '\" + df[\"text\"] + \"' reflects : the moral value \" + df[\"moralstrength\"]+ \" and the emotion \"+ df[\"emotion_word\"] # PROMPT 2\n",
    "        elif prompt_number == 3:\n",
    "            df[\"text\"] = \"the moral value: \" + df[\"moralstrength\"] +\" and the emotion \" + df[\"emotion_word\"]+ \" is reflected in the text '\" + df[\"text\"] + \" '\"   # PROMPT 3\n",
    "        elif prompt_number == 4:\n",
    "            df[\"text\"] = \"The text '\" + df[\"text\"]+ \"' reflects varying intensities of morality such as: \" + df[\"moralstrength_i\"] +\" and different emotions such as: \" +df['depechemood']         # PROMPT 4\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21cdec7-143e-41a2-9b9b-fcdce6cfc729",
   "metadata": {},
   "source": [
    "# Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f110fc8-e845-4160-b33f-91d34eff4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPress task Train and Test Function\n",
    "\n",
    "def train_test_binary(df, m, model_name, directory, name,prompt,lexicon,undersampling=True):\n",
    "    #-----info-----\n",
    "    print(f'Info: {name}')\n",
    "    print(f'Moral to detect: {m}')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Directory: {directory}')\n",
    "    print(f'Undersampling: {undersampling}')\n",
    "    print('Data example:')\n",
    "    print(df.loc[0].text)\n",
    "    \n",
    "    #-----data preparation-----\n",
    "    df = preprocess_data_binary(df, m)\n",
    "    id2label = {0: \"NO-MORAL\", 1: \"MORAL\"}\n",
    "    label2id = {\"NO-MORAL\": 0, \"MORAL\": 1}\n",
    "    \n",
    "    if undersampling:\n",
    "        df=undersampling_data(df)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    #-----info-----\n",
    "    file_name = f\"result_binary_{name}_{lexicon}_moral{m}_prompt{prompt}\"\n",
    "    print(f'File: {file_name}')\n",
    "    print(f'Labels :{label2id}')\n",
    "\n",
    "        \n",
    "    #-----split data-----\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "    datasets = DatasetDict(datasets)\n",
    "\n",
    "    #----model--------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "    \n",
    "    #-----train-----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics)\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    trainer.train()\n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = (end_time - start_time) \n",
    "    #time_results = {\"elapsed_time_train\": elapsed_time}\n",
    "\n",
    "    \n",
    "    #-----inference-----\n",
    "    #start_time_pred = time.time()\n",
    "    predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "    actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "    results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "    #end_time_pred = time.time()\n",
    "    #elapsed_time_pred = (end_time_pred - start_time_pred)\n",
    "    #time_results_pred = {\"elapsed_time prediction\": elapsed_time_pred}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #-----save metrics-----   \n",
    "    results_directory = os.path.join(directory, file_name)\n",
    "    os.makedirs(results_directory, exist_ok=True)\n",
    "    with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "        \n",
    "    #-----save times-----  \n",
    "    #time_file_path = os.path.join(results_directory, f'training_time_{model_name}.json')\n",
    "    #with open(time_file_path, 'w') as f:\n",
    "        #json.dump([time_results,time_results_pred], f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf7679-457c-4306-a13d-6dae6beac636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPol task Train and Test Function\n",
    "\n",
    "def train_test_model_polarity(df, task, model_name, directory, name,prompt,lexicon,undersampling=False):\n",
    "    #-----info-----\n",
    "    print(f'Info: {name}')\n",
    "    print(f'Task: {task}')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Directory: {directory}')\n",
    "    print(f'Undersampling: {undersampling}')\n",
    "    print('Data example:')\n",
    "    print(df.loc[0].text)\n",
    "    \n",
    "    #-----data preparation-----    \n",
    "    df,id2label,label2id = preprocess_data_polarity(df, task)\n",
    "    if undersampling:\n",
    "        df=undersampling_data(df)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #-----info-----\n",
    "    file_name = f\"result_polarity_{name}_{lexicon}_task{task}_prompt{prompt}\"\n",
    "    print(f'File: {file_name}')\n",
    "    print(f'Labels :{label2id}')\n",
    "    \n",
    "        \n",
    "    #-----split data-----\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "    datasets = DatasetDict(datasets)\n",
    "\n",
    "    #-----model------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "    \n",
    "    #-----train-----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    #start_time = time.time()\n",
    "    trainer.train()\n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = (end_time - start_time) \n",
    "    #time_results = {\"elapsed_time_train\": elapsed_time}\n",
    "\n",
    "    #-----inference-----\n",
    "    #start_time_pred = time.time()\n",
    "    predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "    actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "    results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "    #end_time_pred = time.time()\n",
    "    #elapsed_time_pred = (end_time_pred - start_time_pred)\n",
    "    #time_results_pred = {\"elapsed_time prediction\": elapsed_time_pred}\n",
    "\n",
    "    #-----save metrics-----\n",
    "    results_directory = os.path.join(directory, file_name)\n",
    "    os.makedirs(results_directory, exist_ok=True)\n",
    "    with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "        \n",
    "    #-----save times-----  \n",
    "    #time_file_path = os.path.join(results_directory, f'training_time_{model_name}.json')\n",
    "    #with open(time_file_path, 'w') as f:\n",
    "        #json.dump([time_results,time_results_pred], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff51c01-bc3a-4565-a1f0-8ebdf4bcbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiPres Task Train and Test Function\n",
    "\n",
    "def train_test_model_multiclass_6(df, task, model_name, directory, name,prompt,lexicon,undersampling=False):\n",
    "    #-----info-----\n",
    "    print(f'Info: {name}')\n",
    "    print(f'Pask: {task}')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Directory: {directory}')\n",
    "    print(f'Undersampling: {undersampling}')\n",
    "    print('Data example:')\n",
    "    print(df.loc[0].text)\n",
    "    \n",
    "    #------data preparation-----\n",
    "    df = label_multiclass6(df)\n",
    "    id2label,label2id = multiclass_task_6()\n",
    "    if undersampling:\n",
    "        df=undersampling_data(df)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    #-----info-----\n",
    "    file_name = f\"result_multiclass_{name}_{lexicon}_prompt{prompt}\"\n",
    "    print(f'File: {file_name}')\n",
    "    print(f'Labels :{label2id}')\n",
    "\n",
    "\n",
    "    #-----split data-----\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "    datasets = DatasetDict(datasets)\n",
    "\n",
    "    #-----model-----\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6, id2label=id2label, label2id=label2id)\n",
    "\n",
    "    \n",
    "    #-----train-----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    #start_time = time.time()\n",
    "    trainer.train()\n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = ( end_time - start_time) \n",
    "    #time_results = {\"elapsed_time_train\": elapsed_time}\n",
    "\n",
    "\n",
    "    #-----inference-----\n",
    "\n",
    "    #start_time_pred = time.time()\n",
    "    predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "    actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "    results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "    #end_time_pred = time.time()\n",
    "    #elapsed_time_pred = (end_time_pred - start_time_pred) \n",
    "    #time_results_pred = {\"elapsed_time prediction\": elapsed_time_pred}\n",
    "\n",
    "    #-----save metrics-----\n",
    "    results_directory = os.path.join(directory, file_name)\n",
    "    os.makedirs(results_directory, exist_ok=True)\n",
    "    with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "        \n",
    "    #-----save times-----  \n",
    "    #time_file_path = os.path.join(results_directory, f'training_time_{model_name}.json')\n",
    "    #with open(time_file_path, 'w') as f:\n",
    "        #json.dump([time_results,time_results_pred], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dea5a9-e85b-4d58-a7ac-f939e347fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiPol Task Train and Test Function\n",
    "\n",
    "\n",
    "def train_test_model_multiclass_11(df, task, model_name, directory, name,prompt,lexicon,undersampling=False):\n",
    "    #-----info-----\n",
    "    print(f'Info: {name}')\n",
    "    print(f'Task: {task}')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Directory: {directory}')\n",
    "    print(f'Undersampling: {undersampling}')\n",
    "    print('Data example')\n",
    "    print(df.loc[0].text)\n",
    "    \n",
    "    #-----data preparation-----\n",
    "    df = label_multiclass11(df)\n",
    "    id2label,label2id = multiclass_task_11()\n",
    "    if undersampling:\n",
    "        df=undersampling_data(df)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    #-----info-----\n",
    "    file_name = f\"result_multiclass_{name}_{lexicon}_prompt{prompt}\"\n",
    "    print(f'File: {file_name}')\n",
    "    print(f'Labels :{label2id}')\n",
    "\n",
    "    \n",
    "    #-----split data-----\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    datasets = {'train': Dataset.from_pandas(train_df), 'val': Dataset.from_pandas(val_df), 'test': Dataset.from_pandas(test_df)}\n",
    "    datasets = DatasetDict(datasets)\n",
    "\n",
    "    #------model-----\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "    tokenized_datasets = datasets.map(tokenize_function, batched=True) \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=11, id2label=id2label, label2id=label2id)\n",
    "    \n",
    "    #-----train-----\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    trainer.train()\n",
    "    #end_time = time.time()\n",
    "    #elapsed_time = (end_time - start_time)\n",
    "    #time_results = {\"elapsed_time_train\": elapsed_time}\n",
    "\n",
    "\n",
    "    #-----inference-----\n",
    "    start_time_pred = time.time()\n",
    "    predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predicted_class_ids = predictions.predictions.argmax(axis=1)\n",
    "    actual_labels = tokenized_datasets[\"test\"][\"label\"]\n",
    "    results = classification_report(actual_labels, predicted_class_ids, digits=5, output_dict=True)\n",
    "    #end_time_pred = time.time()\n",
    "    #elapsed_time_pred = (end_time_pred - start_time_pred) / 60\n",
    "    #time_results_pred = {\"elapsed_time prediction\": elapsed_time_pred}\n",
    "\n",
    "    #-----save metrics-----\n",
    "    results_directory = os.path.join(directory, file_name)\n",
    "    os.makedirs(results_directory, exist_ok=True)\n",
    "    with open(os.path.join(results_directory, f'results_classification_{model_name}.json'), 'w') as f:\n",
    "        json.dump(results, f)\n",
    "        \n",
    "    #-----save times-----  \n",
    "    #time_file_path = os.path.join(results_directory, f'training_time_{model_name}.json')\n",
    "    #with open(time_file_path, 'w') as f:\n",
    "        #json.dump([time_results,time_results_pred], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cc58b-5a06-4a5e-b495-b2a6e78c8337",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14a9894-a840-45ea-aa11-a06189372132",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='Results/Roberta_best_results'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = directory,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size = 32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy='no')\n",
    "\n",
    "\n",
    "models=['bert-base-uncased','roberta-base']\n",
    "#select model\n",
    "model_name=models[1]\n",
    "\n",
    "#select prompt\n",
    "prompts=['noprompt',1,2,3,4]\n",
    "prompt_number=prompts[2]\n",
    "\n",
    "#select lexicon\n",
    "lexicons=['baseline','moralstrength','depechemood','moralstrength+depechemood']\n",
    "lexicon=lexicons[3]\n",
    "\n",
    "\n",
    "#model tokenizer\n",
    "if model_name=='bert-base-uncased':\n",
    "    folder='Bert'\n",
    "elif model_name=='roberta-base':\n",
    "    folder='Roberta' \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86546ab4-c5c2-4525-8a12-723148f6dc45",
   "metadata": {},
   "source": [
    "# Select datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770eff12-3138-47ef-9245-f7750ca50c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BALTIMORE', 'ALM', 'REDDIT', 'SANDY', 'BLM', 'ELECTION', 'DAVIDSON']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#create datasets\n",
    "folder_path='DATASETS'\n",
    "datasets,filenames,names=load_data(prompt_number,folder_path,lexicon,'_dataset.csv')\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fb1c7-cadd-457b-b627-efdcbfc19b7f",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55825a5c-c86d-485a-9606-d9b18bee848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Moral Estimation Task : MPres, MPol, MultiPres, MultiPol \n",
    "case='MPres'\n",
    "\n",
    "if case== 'MPres':\n",
    "    directory = f\"Results_{folder}_B/MFTD_Dataset_balanced\"\n",
    "    for task in range(1, 6):\n",
    "        for f in names:\n",
    "            df = datasets[f]\n",
    "            print(f'An example: {df.text.loc[0]}')\n",
    "            train_test_model_binary(df.copy(), task, model_name, directory, name= f,prompt=prompt_number,lexicon=lexicon, undersampling=True)\n",
    "            \n",
    "                                \n",
    "elif case=='Mpol':\n",
    "    directory = f\"Results_{folder}_P/MFTD_Dataset_balanced\"\n",
    "    for task in range(1, 6):\n",
    "        for f in names:\n",
    "            df = datasets[f]\n",
    "            df['label']=df['label_annotators']\n",
    "            print(f'An example: {df.text.loc[0]}')\n",
    "            df.label=df.label.replace({\"hate\": 'degradation','Care':'care','Harm':'harm','Fairness':'fairness','Cheating':'cheating','Loyalty':'loyalty','Betrayal':'betrayal','Authority':'authority','Subversion':'subversion','Purity':'purity','Degradation':'degradation','Non-moral':'non-moral','nm':'non-moral','Non-Moral':'non-moral'})\n",
    "            train_test_model_polarity(df.copy(), task, model_name, directory, name=f,prompt=prompt_number,lexicon=lexicon,undersampling=True)\n",
    "\n",
    "\n",
    "elif case=='MultiPres':\n",
    "    directory = f\"Results_{folder}_M6/MFTD_Dataset_balanced\"\n",
    "    for f in names:\n",
    "        df = datasets[f]\n",
    "        print(f'An example: {df.text.loc[0]}')\n",
    "        train_test_model_multiclass_5(df.copy(), 1, model_name, directory, name=f,prompt=prompt_number,lexicon=lexicon,undersampling=True)\n",
    "\n",
    "\n",
    "elif case=='MultiPol':\n",
    "    directory = f\"Results_{folder}_M11/MFTD_Dataset_balanced\"\n",
    "    for f in names:\n",
    "        df = datasets[f]\n",
    "        df['label']=df['label_annotators']\n",
    "        df.label=df.label.replace({\"hate\": 'degradation','Care':'care','Harm':'harm','Fairness':'fairness','Cheating':'cheating','Loyalty':'loyalty','Betrayal':'betrayal','Authority':'authority','Subversion':'subversion','Purity':'purity','Degradation':'degradation','Non-moral':'non-moral','nm':'non-moral','Non-Moral':'non-moral'})\n",
    "        train_test_model_multiclass_11(df.copy(), 1, model_name, directory, name=f,prompt=prompt_number,lexicon=lexicon,undersampling=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
